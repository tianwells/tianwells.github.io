---
layout: post
title: Week 2
---
My work this week was centered around live captioning and the different types of caption metrics out there. I feel that I made good progress in solidifying our research question and working out any uncertain details surrounding it. At the start of the week, our research question was “How well does the WER/WWER assess quality compared to the NER model? How should we define quality? (user experience, industry applicability, etc)”, and by the end of the week it was “How well/accurately do the WER/WWER/NER/ACE models assess quality compared to each other?” I focused on NER captioning at first and researched that extremely in depth, but I found out you need to be trained to evaluate NER, so I left that topic alone to research other metrics. I found out about ACE, and looked into that. In trying to make sure our category of research hasn’t already been done, I looked for research out there that is similar to our research question. I found one article that compared the WER and ACE metrics already and found that ACE did better in comparison to WER, but I learned that metrics being used on videos is much different than metrics being used on live TV, so our research question is still new and valid. Next week, I’m excited to look more into how we’ll set up our procedures, and to get into any coding that we will need to do for the project. 
